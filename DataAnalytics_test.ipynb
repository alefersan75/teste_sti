{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eec3119",
   "metadata": {},
   "source": [
    "# ST IT Cloud - Data and Analytics Test LV.4\n",
    "\n",
    "Esse teste deve avaliar alguns conceitos de big data e a qualidade técnica na manipulacão de dados, otimização de performance, trabalho com arquivos grandes e tratamento de qualidade.\n",
    "\n",
    "## Passo a passo\n",
    "\n",
    "- *Parte teórica:* responda as questões abaixo preenchendo as células em branco.\n",
    "- *Parte prática:* disponibilizamos aqui 2 cases para, leia os enunciados dos problemas, desenvolver os programas, utilizando a **stack definida durante o processo seletivo**, para entregar os dados de acordo com os requisitos descritos abaixo.\n",
    "\n",
    "\n",
    "\n",
    "**Faz parte dos critérios de avaliacão a pontualidade da entrega. Implemente até onde for possível dentro do prazo acordado.**\n",
    "\n",
    "**Os dados de pessoas foram gerados de forma aleatória, utilizando a biblioteca FakerJS, FakerJS-BR e Faker**\n",
    "\n",
    "LEMBRE-SE: A entrega deve conter TODOS os passos para o avaliador executar o programa (keep it simple).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447dec4",
   "metadata": {},
   "source": [
    "**Questão 1** - Descreva de forma detalhada quais são as etapas na construção de um pipeline de dados, sem considerar ferramentas específicas, imagine que é seu primeiro contato com o cliente e você precisa entender a demanda dele e explicar quais são os passos que você terá que implementar para entregar a demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources\n",
    "Esta parte é onde tudo começa, de onde vêm as informações e pode envolver diferentes fontes, como APIs, nuvem, bancos de dados relacionais, NoSQL e Apache Hadoop.\n",
    "\n",
    "Ingressos\n",
    "Os dados de diferentes fontes são frequentemente combinados à medida que viajam pelo pipeline. As junções listam os critérios e a lógica de como esses dados são agrupados.\n",
    "\n",
    "Padronização\n",
    "A ideia é garantir que todos os dados sigam as mesmas unidades de medida e sejam apresentados em tamanho, fonte e cor aceitáveis.\n",
    "\n",
    "Correção\n",
    "Onde tem dados pode haver erros. Pode ser algo tão simples como um cep que não existe ou uma sigla confusa. A fase de correção também remove registros com problemas.\n",
    "\n",
    "Cargas\n",
    "Depois que os dados são limpos, eles são carregados, geralmente um data warehouse, outro banco de dados relacional ou uma estrutura Hadoop.\n",
    "\n",
    "Automação\n",
    "Os pipelines de dados empregam o processo de automação continuamente ou em uma programação. O processo de automação lida com a detecção de erros, relatórios de status e monitoramento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc703af3",
   "metadata": {},
   "source": [
    "**Questão 2** - Defina com suas palavras um processamento em streaming e processamento em batch. Qual sua experiência com cada uma delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Em resumo,\n",
    "O processamento em lote envolve a manipulação de blocos de dados que já foram armazenados durante um determinado período de tempo.\n",
    "O processamento de streaming executa operações em dados em movimento ou em tempo real.\n",
    "Muito interessante contar uma experiência que tive ao trabalhar com dados de telefonia, onde através de canais E1, na medida que \n",
    "recebíamos muitas ligações, decorrente de envios de SMS, solicitando que entrassem em contato, tínhamos um motor\n",
    "de ETL, que controlava o envio de sms, se por um determinado tempo parava de receber ligações, aumentávamos o envio de sms.\n",
    "Além de pontuar a carteira de clientes com dados de telefones com mais qualidade em tempo real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c3f10",
   "metadata": {},
   "source": [
    "**Questão 3** - Quais são as camadas de um Data Lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b803419",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ingestão, armazenamento em cache e processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62477089",
   "metadata": {},
   "source": [
    "**Questão 4** - Quais as diferenças de um Data Lake e um DW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A primeira grande diferença entre um data lake e data warehouse é o tipo de dados que são armazenados dentro deles. Enquanto um Data Lake armazena qualquer tipo de dados, incluindo arquivos, logs, imagens ou dados de sensores, por exemplo, o Data Warehouse armazena primariamente dados estruturados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099a05e",
   "metadata": {},
   "source": [
    "**Questão 5** - O que é arquitetura Lambda e Kappa? Descreva com suas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4347923",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda\n",
    "Todos os dados recebidos pelo sistema passam por uma camada de lote e uma camada de velocidade.\n",
    "\n",
    "Kappa\n",
    "Ela tem as mesmas metas básicas da arquitetura de lambda, mas com uma diferença importante, todos os dados fluem por um único caminho, usando um sistema de processamento de fluxo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d483e",
   "metadata": {},
   "source": [
    "**Questão 6** - O que é Data Quality para você e como você implementa isso nos seus processos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "É um conjunto de processos que visa garantir que os dados armazenados sejam corretos, precisos,consistentes, completos e integrados.\n",
    "A Implementação dar-se-a garantir a qualidade de dados, como: rastrear a origem das informações, realizar verificações e atualizações periódicas, e implementar, configurar e integrar ferramentas de apoio para a análise de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4834d6",
   "metadata": {},
   "source": [
    "**Questão 7** - Em uma escala de 0 a 10, qual seria seu nível de experiência com PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Muito pouco, quase zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef78ba",
   "metadata": {},
   "source": [
    "**Questão 8** - Em uma escala de 0 a 10, qual seria seu nível de experiência com SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33569bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Posso dizer que me viro bem, mas sei que teria muito a aprender. Nota 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f7ee6",
   "metadata": {},
   "source": [
    "**Questão 9** - Descreva suas expeciências com banco de dados SQL e NoSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ja utilizei MYSQL, desde a configuração e instalação em linux com engine MYISAM, INNODB, INFINIDB (Banco analitico). SQLServer, MariaDB com infiniDB, MongoDB conectado com Pentaho. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa6e5",
   "metadata": {},
   "source": [
    "**Questão 10** - Tem experiência com versionamento de código? Com quais ferramentas já trabalhou? Descreva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim. Ferramentas de git, github, gitlab e bitbucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2714f",
   "metadata": {},
   "source": [
    "**Questão 11** - Tem experiência em desenvolvimento em cloud? Se sim, especifique a(s) plataforma(s) que já trabalhou e suas principais implementações e conhecimentos em cada serviço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trabalhei com a AWS, na utilizacao de servidores linux. Adquiria de forma limpa, e realizava as instalações de banco de dados, configurava firewall, vpn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc70d",
   "metadata": {},
   "source": [
    "**Questão 12** - Tem experiência com metodologia ágil? Qual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5be8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim. Scrum e Kanban "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb61f06",
   "metadata": {},
   "source": [
    "# TESTE PRÁTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8c6a5",
   "metadata": {},
   "source": [
    "**Problema 1**: Você está recebendo o arquivo 'dados_cadastrais_fake.csv' que contem dados cadastrais de clientes, mas para que análises ou relatórios sejam feitos é necessário limpar e normalizar os dados. Além disso, existe uma coluna com o número de cpf e outra com cnpj, você precisará padronizar deixando apenas dígitos em formato string (sem caracteres especiais), implementar uma forma de verificar se tais documentos são válidos sendo que a informação deve se adicionada ao dataframe em outras duas novas colunas.\n",
    "\n",
    "Após a normalização, gere reports que respondam as seguintes perguntas:\n",
    "- Quantos clientes temos nessa base?\n",
    "- Qual a média de idade dos clientes?\n",
    "- Quantos clientes nessa base pertencem a cada estado?\n",
    "- Quantos CPFs válidos e inválidos foram encontrados?\n",
    "- Quantos CNPJs válidos e inválidos foram encontrados?\n",
    "\n",
    "Ao final gere um arquivo no formato csv e um outro arquivo no formato parquet chamado (problema1_normalizado), eles serão destinados para pessoas distintas.\n",
    "\n",
    "*EXTRA:* executar as mesmas validações no *1E8.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dados_cadastrais_fake.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da9c40",
   "metadata": {},
   "source": [
    "**Problema 2**: Você deverá implementar um programa, para ler, tratar e particionar os dados.\n",
    "\n",
    "O arquivo fonte está disponível em `https://st-it-cloud-public.s3.amazonaws.com/people-v2_1E6.csv.gz`\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Higienizar e homogenizar o formato da coluna `document`\n",
    "- Detectar através da coluna `document` se o registro é de uma Pessoa Física ou Pessoa Jurídica, adicionando uma coluna com essa informação\n",
    "- Higienizar e homogenizar o formato da coluna `birthDate`\n",
    "- Existem duas colunas nesse dataset que em alguns registros estão trocadas. Quais são essas colunas? \n",
    "- Corrigir os dados com as colunas trocadas\n",
    "- Além desses pontos, existem outras tratamentos para homogenizar esse dataset. Aplique todos que conseguir.\n",
    "\n",
    "### Agregação dos dados\n",
    "\n",
    "- Quais são as 5 PF que mais gastaram (`totalSpent`)? \n",
    "- Qual é o valor de gasto médio por estado (`state`)?\n",
    "- Qual é o valor de gasto médio por `jobArea`?\n",
    "- Qual é a PF que gastou menos (`totalSpent`)?\n",
    "- Quantos nomes e documentos repetidos existem nesse dataset?\n",
    "- Quantas linhas existem nesse dataset?\n",
    "\n",
    "### Particionamento de dados tratados com as regras descritas em `DATA QUALITY`\n",
    "\n",
    "- Particionar em arquivos PARQUET por estado (`state`)\n",
    "- Particionar em arquivos CSV por ano/mes/dia de nascimento (`birthDate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277f816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
